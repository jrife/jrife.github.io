---
layout: post
title: How I Became a Top Contributor to The 6.10 Kernel
tags: [kubernetes, kernel, ebpf, networking, storage]
image: '/images/posts/2.jpeg'
---

OK, while this is *technically* true based on the [published](https://lwn.net/Articles/981559/)
statistics for the 6.10 Linux kernel, I'm hardly a core contribtor. But hey,
I wanted a catchy title.

![alt](/images/posts/6.10-contributors.png)

In 6.10 I made a large number of changes to the kernel selftests for BPF
sockaddr hooks to add regression coverage for a family of issues I fixed in
prior releases, so lots of ctrl-c+ctrl-v and refactoring of existing test code 
([1](https://lore.kernel.org/bpf/20240510190246.3247730-1-jrife@google.com/T/#u),
[2](https://lore.kernel.org/bpf/20240429214529.2644801-1-jrife@google.com/T/#u)).

In this post we'll take a look at these issues, how I fixed them, and find out
a bit more about the interactions between Kubernetes, Cilium, eBPF, and Linux
Kernel, and software-defined storage such as Portworx, Longhorn, etc.

##### The Problem

Storage is an important component in many Kubernetes setups. There are a wide
variety of [CSI](https://kubernetes-csi.github.io/docs/) drivers that can
provide persistent storage to your cluster. These days, most storage appliance
vendors provide some CSI driver that enables Kubernetes clusters to connect to
their storage system. Some examples include:

* [Trident](https://docs.netapp.com/us-en/netapp-solutions/containers/rh-os-n_overview_trident.html)
  is NetApp's CSI driver.
* [CSI Powerstore](https://github.com/dell/csi-powerstore) is the CSI driver for
  Dell Powerstore.
* [Synology CSI](https://github.com/SynologyOpenSource/synology-csi) is the CSI
  driver for Synology NAS.

These drivers manage a storage appliance on behalf of a Kubernetes cluster to
make sure that `PersistentVolumeClaims`, `VolumeSnapshots`, etc. *just work*.

*Software-defined* storage systems such as [Longhorn](https://longhorn.io/),
[Portworx](https://portworx.com/), and [Robin](https://docs.robin.io/) provide
persistent storage to a Kubernetes cluster without a dedicated external storage
appliance. With these systems, data often lives on the the cluster nodes with
the storage system handling persistence, replication, and availability of data
across the cluster.

In Kubernetes volumes have an [*access mode*](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
which specifies how a volume can be mounted. For the purposes of this discussion
we'll focus on `ReadWriteMany` volumes and how they are implemented in these
software-defined storage systems.

> **ReadWriteMany**
>
> the volume can be mounted as read-write by many nodes.

Simply put, `ReadWriteMany` volumes are volumes that I can read and write to
from multiple nodes in my cluster at the same time. In theory this says nothing
about the implementation. In practice this usually means NFS, SMB, or some
distributed file system under the hood. *Technically* you can also provision and
use a `ReadWriteMany` volume with `volumeMode: Block` provided that the
underlying CSI driver supports it which allows multiple nodes to access a raw
block device, but this is much less common and typically access would need to be
orchestrated at the application level to prevent toe stepping.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
```

All of the software-defined storage systems listed above implement
`ReadWriteMany` storage by creating up a userspace NFS server
(e.g. [`nfs-ganesha`](https://github.com/nfs-ganesha/nfs-ganesha)) as a pod
beind a clusterIP [service](https://kubernetes.io/docs/concepts/services-networking/service/).

![alt](/images/posts/sds.png)

In this architecture the service is used purely to provide a stable IP address
for the NFS mounts. If the NFS server pod needs to be replaced by the storage
system due to node restarts, pod deletions, etc. this allows any mounts to
recover as reconnection attempts on the client will be redirected to the new
backend.

TODO
