---
layout: post
title: How I Became a Top Contributor to The 6.10 Kernel
tags: [kubernetes, kernel, ebpf, networking, storage]
image: '/images/posts/2.jpeg'
---

OK, while this is *technically* true based on the [published](https://lwn.net/Articles/981559/)
statistics for the 6.10 Linux kernel, I'm hardly a core contribtor. But hey,
I wanted a catchy title.

![alt](/images/posts/6.10-contributors.png)

In 6.10 I made a large number of changes to the kernel selftests for BPF
sockaddr hooks to add regression coverage for a family of issues I fixed in
prior releases, so lots of ctrl-c+ctrl-v and refactoring of existing test code 
([1](https://lore.kernel.org/bpf/20240510190246.3247730-1-jrife@google.com/T/#u),
[2](https://lore.kernel.org/bpf/20240429214529.2644801-1-jrife@google.com/T/#u)).

In this post we'll take a look at these issues, how I fixed them, and find out
a bit more about the interactions between Kubernetes, Cilium, eBPF, and Linux
Kernel, and software-defined storage such as Portworx, Longhorn, etc.

##### The Problem

Storage is an important component in many Kubernetes setups. There are a wide
variety of [CSI](https://kubernetes-csi.github.io/docs/) drivers that can
provide persistent storage to your cluster. These days, most storage appliance
vendors provide some CSI driver that enables Kubernetes clusters to connect to
their storage system. Some examples include:

* [Trident](https://docs.netapp.com/us-en/netapp-solutions/containers/rh-os-n_overview_trident.html)
  is NetApp's CSI driver.
* [CSI Powerstore](https://github.com/dell/csi-powerstore) is the CSI driver for
  Dell Powerstore.
* [Synology CSI](https://github.com/SynologyOpenSource/synology-csi) is the CSI
  driver for Synology NAS.

These drivers manage a storage appliance on behalf of a Kubernetes cluster to
make sure that `PersistentVolumeClaims`, `VolumeSnapshots`, etc. *just work*.

*Software-defined* storage systems such as [Longhorn](https://longhorn.io/),
[Portworx](https://portworx.com/), and [Robin](https://docs.robin.io/) provide
persistent storage to a Kubernetes cluster without a dedicated external storage
appliance. With these systems, data often lives on the the cluster nodes with
the storage system handling persistence, replication, and availability of data
across the cluster.

In Kubernetes volumes have an [*access mode*](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
which specifies how a volume can be mounted. For the purposes of this discussion
we'll focus on `ReadWriteMany` volumes and how they are implemented in these
software-defined storage systems.

> **ReadWriteMany**
>
> the volume can be mounted as read-write by many nodes.

Simply put, `ReadWriteMany` volumes are volumes that I can read and write to
from multiple nodes in my cluster at the same time. In theory this says nothing
about the implementation. In practice this usually means NFS, SMB, or some
distributed file system under the hood. *Technically* you can also provision and
use a `ReadWriteMany` volume with `volumeMode: Block` provided that the
underlying CSI driver supports it which allows multiple nodes to access a raw
block device, but this is much less common and typically access would need to be
orchestrated at the application level to prevent toe stepping.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
```

All of the software-defined storage systems listed above implement
`ReadWriteMany` storage by creating up a userspace NFS server
(e.g. [nfs-ganesha](https://github.com/nfs-ganesha/nfs-ganesha)) as a pod
beind a clusterIP [service](https://kubernetes.io/docs/concepts/services-networking/service/).

![alt](/images/posts/sds.png)

In this architecture the service is used purely to provide a stable IP address
for the NFS mounts. If the NFS server pod needs to be replaced by the storage
system due to node restarts, pod deletions, etc. this allows any mounts to
recover as reconnection attempts on the client will be redirected to the new
backend.

![alt](/images/posts/failover.png)

Traditionally, clusterIP services were implemented by [kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/),
a daemon that runs on each node in the cluster. kube-proxy creates iptables
rules for each service that DNAT a service's clusterIP (e.g. `10.0.0.80` in the
diagram above) to an endpoint IP (e.g. `192.168.0.54`). Netfilter then replaces
the service IP with an endpoint IP for each packet. However, this isn't the
only way to implement this concept. [Cilium](https://cilium.io/)'s socket-LB
feature implements cluster IP services using [eBPF](https://ebpf.io/) hooks
that execute on the `connect()`, `sendmsg()`, and `recvmsg()` system calls to
rewrite the address parameter. Using this approach the socket is connected
directly to a backend address, requiring no per-packet DNAT. We'll delve a bit
more into eBPF, these hooks, and their implementation in the Linux kernel below.

When disconnected from an NFS server, Linux periodically attempts to reconnect
to recover the NFS mount. When the NFS server address is a service IP
reconnection attempts *should* be directed to the new backend once it's ready.
This worked fine with kube-proxy or when using Cilium *without* socket-LB.
However, *with* socket-LB NFS mounts would get stuck if the original backend
was replaced [1](https://github.com/cilium/cilium/issues/21541). This turned out
to be caused by a deeper problem within the Linux kernel impacting the
interactions between the NFS (SUNRPC) driver and eBPF socket syscall hooks.

##### Exploring The Kernel

###### eBPF + Socket Syscalls

[`connect()`](https://github.com/torvalds/linux/blob/a0e3919a2df29b373b19a8fbd6e4c4c38fc10d87/net/socket.c#L2077)

```c
/*
 *	Attempt to connect to a socket with the server address.  The address
 *	is in user space so we verify it is OK and move it to kernel space.
 *
 *	For 1003.1g we need to add clean support for a bind to AF_UNSPEC to
 *	break bindings
 *
 *	NOTE: 1003.1g draft 6.3 is broken with respect to AX.25/NetROM and
 *	other SEQPACKET protocols that take time to connect() as it doesn't
 *	include the -EINPROGRESS status for such sockets.
 */

int __sys_connect_file(struct file *file, struct sockaddr_storage *address,
		       int addrlen, int file_flags)
{
	struct socket *sock;
	int err;

	sock = sock_from_file(file);
	if (!sock) {
		err = -ENOTSOCK;
		goto out;
	}

	err =
	    security_socket_connect(sock, (struct sockaddr *)address, addrlen);
	if (err)
		goto out;

	err = READ_ONCE(sock->ops)->connect(sock, (struct sockaddr *)address,
				addrlen, sock->file->f_flags | file_flags);
out:
	return err;
}

int __sys_connect(int fd, struct sockaddr __user *uservaddr, int addrlen)
{
	int ret = -EBADF;
	struct fd f;

	f = fdget(fd);
	if (fd_file(f)) {
		struct sockaddr_storage address;

		ret = move_addr_to_kernel(uservaddr, addrlen, &address);
		if (!ret)
			ret = __sys_connect_file(fd_file(f), &address, addrlen, 0);
		fdput(f);
	}

	return ret;
}

SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,
		int, addrlen)
{
	return __sys_connect(fd, uservaddr, addrlen);
}
```

`inet_stream_connect()`

```c
/*
 *	Connect to a remote host. There is regrettably still a little
 *	TCP 'magic' in here.
 */
int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
			  int addr_len, int flags, int is_sendmsg)
{
	struct sock *sk = sock->sk;
	int err;
	long timeo;

	/*
	 * uaddr can be NULL and addr_len can be 0 if:
	 * sk is a TCP fastopen active socket and
	 * TCP_FASTOPEN_CONNECT sockopt is set and
	 * we already have a valid cookie for this socket.
	 * In this case, user can call write() after connect().
	 * write() will invoke tcp_sendmsg_fastopen() which calls
	 * __inet_stream_connect().
	 */
	if (uaddr) {
		if (addr_len < sizeof(uaddr->sa_family))
			return -EINVAL;

		if (uaddr->sa_family == AF_UNSPEC) {
			sk->sk_disconnects++;
			err = sk->sk_prot->disconnect(sk, flags);
			sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
			goto out;
		}
	}

	switch (sock->state) {
	default:
		err = -EINVAL;
		goto out;
	case SS_CONNECTED:
		err = -EISCONN;
		goto out;
	case SS_CONNECTING:
		if (inet_test_bit(DEFER_CONNECT, sk))
			err = is_sendmsg ? -EINPROGRESS : -EISCONN;
		else
			err = -EALREADY;
		/* Fall out of switch with err, set for this state */
		break;
	case SS_UNCONNECTED:
		err = -EISCONN;
		if (sk->sk_state != TCP_CLOSE)
			goto out;

		if (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {
			err = sk->sk_prot->pre_connect(sk, uaddr, addr_len);
			if (err)
				goto out;
		}

		err = sk->sk_prot->connect(sk, uaddr, addr_len);
		if (err < 0)
			goto out;

		sock->state = SS_CONNECTING;

		if (!err && inet_test_bit(DEFER_CONNECT, sk))
			goto out;

		/* Just entered SS_CONNECTING state; the only
		 * difference is that return value in non-blocking
		 * case is EINPROGRESS, rather than EALREADY.
		 */
		err = -EINPROGRESS;
		break;
	}

	timeo = sock_sndtimeo(sk, flags & O_NONBLOCK);

	if ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
		int writebias = (sk->sk_protocol == IPPROTO_TCP) &&
				tcp_sk(sk)->fastopen_req &&
				tcp_sk(sk)->fastopen_req->data ? 1 : 0;
		int dis = sk->sk_disconnects;

		/* Error code is set above */
		if (!timeo || !inet_wait_for_connect(sk, timeo, writebias))
			goto out;

		err = sock_intr_errno(timeo);
		if (signal_pending(current))
			goto out;

		if (dis != sk->sk_disconnects) {
			err = -EPIPE;
			goto out;
		}
	}

	/* Connection was closed by RST, timeout, ICMP error
	 * or another process disconnected us.
	 */
	if (sk->sk_state == TCP_CLOSE)
		goto sock_error;

	/* sk->sk_err may be not zero now, if RECVERR was ordered by user
	 * and error was received after socket entered established state.
	 * Hence, it is handled normally after connect() return successfully.
	 */

	sock->state = SS_CONNECTED;
	err = 0;
out:
	return err;

sock_error:
	err = sock_error(sk) ? : -ECONNABORTED;
	sock->state = SS_UNCONNECTED;
	sk->sk_disconnects++;
	if (sk->sk_prot->disconnect(sk, flags))
		sock->state = SS_DISCONNECTING;
	goto out;
}
EXPORT_SYMBOL(__inet_stream_connect);

int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
			int addr_len, int flags)
{
	int err;

	lock_sock(sock->sk);
	err = __inet_stream_connect(sock, uaddr, addr_len, flags, 0);
	release_sock(sock->sk);
	return err;
}
EXPORT_SYMBOL(inet_stream_connect);
```

`sk->sk_prot->pre_connect()`

```c
		if (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {
			err = sk->sk_prot->pre_connect(sk, uaddr, addr_len);
			if (err)
				goto out;
		}
```

`tcp_v4_pre_connect()`

```c
static int tcp_v4_pre_connect(struct sock *sk, struct sockaddr *uaddr,
			      int addr_len)
{
	/* This check is replicated from tcp_v4_connect() and intended to
	 * prevent BPF program called below from accessing bytes that are out
	 * of the bound specified by user in addr_len.
	 */
	if (addr_len < sizeof(struct sockaddr_in))
		return -EINVAL;

	sock_owned_by_me(sk);

	return BPF_CGROUP_RUN_PROG_INET4_CONNECT(sk, uaddr, &addr_len);
}
```

TODO
